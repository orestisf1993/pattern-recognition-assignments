\chapter{Προεπεξεργασία Δεδομένων}
\section{Μορφή των δεδομένων}
Τα δεδομένα μας προέρχονται από τους όρους-λέξεις 80 βιβλιοθηκών της γλώσσας προγραμματισμού Java που ανήκουν σε 8 κατηγορίες:
\begin{enumerate}\label{itemize:categories}
    \item android
    \item command-line-parsers
    \item csv-libraries
    \item http-clients
    \item json-libraries
    \item swing-libraries
    \item testing-frameworks
    \item xml-processing
\end{enumerate}

Το αρχικό μας dataset περιλαμβάνει $109706$ λέξεις οι οποίες παρουσιάζονται σαν στήλες σε ένα αρχείο \texttt{.csv}.
Κάτω από την καθεμία αναγράφονται οι απόλυτες συχνότητες εμφάνισης του κάθε όρου.
Επίσης, δίνονται οι στήλες \texttt{project}, κάτω από την οποία είναι τα ονόματα της κάθε βιβλιοθήκης, και \texttt{category}, κάτω από την οποία είναι η
\hyperref[itemize:categories]{κατηγορία} που ανήκει η κάθε βιβλιοθήκη.

\begin{center}
\captionsetup{type={table}}
\resizebox{\textwidth}{!}{%
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
project & category & ACONST & CompoundButtonCheckedChangeOnSubscribe & StatisticsComponent & getProjection & hashedSignature & propertyDescs & testGetBound & testReadPaths \\ \hline
com.appnexus.opensdk/appnexus-sdk & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.auth0.android/lock & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.bingzer.android.ads/adrunner & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.daimajia.easing/library & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.facebook.android/facebook-android-sdk & android & 0 & 0 & 0 & 0 & 2 & 0 & 0 & 0 \\
com.facebook.fresco/fbcore & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.facebook.fresco/fresco & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.github.asne/asne-core & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.github.castorflex.smoothprogressbar/library & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.hannesdorfmann.mosby/mvp & android & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
com.jakewharton.rxbinding/rxbinding & android & 0 & 3 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.jakewharton.timber/timber & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.jdroidframework/jdroid-android & android & 0 & 0 & 0 & 3 & 0 & 0 & 0 & 0 \\
com.joanzapata.iconify/android-iconify & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.mixpanel.android/mixpanel-android & android & 0 & 0 & 0 & 0 & 0 & 3 & 0 & 1 \\
com.rengwuxian.materialedittext/library & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.shamanland/xdroid-core & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.stanfy.enroscar/enroscar-beans & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.stanfy.enroscar/enroscar-sdk-dep & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.stanfy.enroscar/enroscar-shared & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
de.quist.apps.maps/android-maps-abstraction & android & 0 & 0 & 0 & 6 & 0 & 0 & 0 & 0 \\
eu.inmite.android.lib/android-styled-dialogs & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
io.palaima.debugdrawer/debugdrawer & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
io.reactivex/rxandroid & android & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
args4j/args4j & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.beust/jcommander & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.github.spullara.cli-parser/cli-parser & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.lexicalscope.jewelcli/jewelcli & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
commons-cli/commons-cli & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
gnu.getopt/java-getopt & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
io.airlift/airline & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sf.jopt-simple/jopt-simple & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sourceforge.argparse4j/argparse4j & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.cyclopsgroup/jcli & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.realityforge.getopt4j/getopt4j & command-line-parsers & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
au.com.bytecode/opencsv & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.fasterxml.jackson.dataformat/jackson-dataformat-csv & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.opencsv/opencsv & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sf.flatpack/flatpack & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sf.supercsv/super-csv-dozer & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sf.supercsv/super-csv & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.apache.commons/commons-csv & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.beanio/beanio & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.jdbi/jdbi & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.onebusaway/onebusaway-csv-entities & csv-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.github.kevinsawicki/http-request & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.google.http-client/google-http-client & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
com.jcabi/jcabi-http & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.m3/curly & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.mashape.unirest/unirest-java & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.metamx/http-client & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.ning/async-http-client & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.squareup.okhttp/okhttp & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.apache.httpcomponents/httpasyncclient & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.apache.httpcomponents/httpclient & http-clients & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.alibaba/fastjson & json-libraries & 5 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.eclipsesource.minimal-json/minimal-json & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.fasterxml.jackson.core/jackson-core & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.google.code.gson/gson & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.googlecode.json-simple/json-simple & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.jayway.jsonpath/json-path & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.minidev/json-smart & json-libraries & 4 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.codehaus.jettison/jettison & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.json/json & json-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.github.rickyclarkson/swingflow & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sf.cssbox/swingbox & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.buildsomethingawesome.lib/awesome-java-swing & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.fuin/utils4swing & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.scijava/swing-checkbox-tree & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.softsmithy.lib/softsmithy-lib-swing & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.swinglabs/swingx & swing-libraries & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.novocode/junit-interface & testing-frameworks & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
junit/junit & testing-frameworks & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
org.testng/testng & testing-frameworks & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
xmlunit/xmlunit & testing-frameworks & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
com.fasterxml/aalto-xml & xml-processing & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
dom4j/dom4j & xml-processing & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
jdom/jdom & xml-processing & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
net.sf.kxml/kxml2 & xml-processing & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
xstream/xstream & xml-processing & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ \hline
\end{tabular}
}
\captionof{table}{Παράδειγμα μερικών στηλών για όλες τις γραμμές του αρχικού dataset}
\label{table:init}
\end{center}

\section{Εξερεύνηση Δεδομένων}
Πριν την διαδικασία της προεπεξεργασίας των δεδομένων, πραγματοποιούμε μία αρχική εξερεύνηση τους για την κατανόηση του προβλήματος ομαδοποίησης που καλούμαστε να αντιμετωπίσουμε.

Το αρχικό σύνολο δεδομένων, που έχουμε στη διάθεσή μας, παρατηρούμε ότι περιλαμβάνει έναν πολύ μεγάλο αριθμό λέξεων που αποτελούν ουσιαστικά τα attributes για τους αλγορίθμους μας.
Για αυτό το λόγο, στόχος μας ήταν η δραστική μείωση του αριθμού αυτών των λέξεων ενώ αποφασίσαμε ότι δεν ήταν σημαντικό να αφαιρέσουμε κάποια βιβλιοθήκη (γραμμή) καθώς ο αριθμός τους ήταν ήδη περιορισμένος.

Επίσης, παρατηρήσαμε ότι υπήρχαν διάφορα προβλήματα πάνω στην ποιότητα των λέξεων του dataset:
\begin{itemize}
    \item Πολλές λέξεις δεν είχαν κάποιο νόημα πχ
    % manual hyphenation because it didn't work with anything else.
    \texttt{ABCDE\-FGHIJKLMNO\-PQRSTU\-VWXYZ\-abc\-d\-e\-f\-ghij\-klmnopqrstuvwxyz}
    που λογικά χρησιμοποιείται ως ένα string που περιέχει όλους τους χαρακτήρες της Αγγλικής γλώσσας.

    \item Πολλές λέξεις υπήρχαν σε διάφορα σημεία αλλά λόγω της διάκρισης κεφαλαίων και πεζών θεωρούνταν ξεχωριστές λέξεις.
    πχ \texttt{ACCESS}, \texttt{access} και \texttt{Access}.

    \item Μερικές λέξεις ήταν παρόμοιες ή η μια ήταν στον ενικό ενώ η άλλη στον πληθυντικό ή υπήρχαν getters και setters.
    πχ \begin{itemize}
        \item \texttt{action} και \texttt{actions}.
        \item \texttt{accounts} και \texttt{getAccounts}.
    \end{itemize}
\end{itemize}

Τελικά, αποφασίσαμε ότι σε πολλές περιπτώσεις είναι πιο σημαντική η ύπαρξη ή όχι μιας λέξης σε μια βιβλιοθήκη παρά τον αριθμό των εμφανίσεών της.
Αυτό συμβαίνει γιατί γενικά το dataset μας είναι σχετικά αραιό.

\section{Η διαδικασία προεπεξεργασίας δεδομένων}
Η επεξεργασία έγινε στο Python
\footnote{Γραμμένο για να δουλεύει με python3 αλλά έγιναν διάφορες αλλαγές ώστε να τρέχει σωστά και σε python2.}
αρχείο \texttt{preprocess.py}.
Το dataset μας αρχικά διαβάζεται από ένα \texttt{.csv} αρχείο και αποθηκεύεται σε ένα αντικείμενο \lstinline!DataFrame! της βιβλιοθήκης \lstinline!pandas!.

Κατά την προεπεξεργασία των δεδομένων ακολουθήθηκε μια δεντρική δομή για την παραγωγή διάφορων τελικών dataset.
Αυτή η δομή προσδιορίζεται από τη συνάρτηση \lstinline!tree_init()!
και μπορεί να γίνει εύκολα η επεξεργασία της ώστε να αλλάξουν τα τελικά αποτελέσματα.

Ρίζα του δέντρου θεωρείται πάντα το αρχικό μας dataset και κάθε μεταβολή του αναπαριστάται σε ένα κόμβο παιδί.
Η κάθε διαδικασία επεξεργασίας αντιστοιχεί σε μία ακμή του δέντρου.
Τα διάφορα dataset που δημιουργούνται αποθηκεύονται σαν αρχεία και πάλι σε δεντρική δομή όπου οι φάκελοι είναι οι ακμές και τα \texttt{.csv} αρχεία οι κόμβοι.
Μια τέτοια δομή φαίνεται παρακάτω:
\begin{Verbatim}[frame=single]
datasets/root
├── join_duplicates
│   ├── frequency_based_selection
│   │   ├── gibberish_detector
│   │   │   ├── bool_it
│   │   │   │   ├── frequency_based_selection2
│   │   │   │   │   └── dataset.csv
│   │   │   │   └── dataset.csv
│   │   │   ├── join_similar
│   │   │   │   ├── drop_fry_words
│   │   │   │   │   ├── frequency_based_selection_df
│   │   │   │   │   │   └── dataset.csv
│   │   │   │   │   └── dataset.csv
│   │   │   │   └── dataset.csv
│   │   │   └── dataset.csv
│   │   └── dataset.csv
│   └── dataset.csv
└── dataset.csv
\end{Verbatim}

Στη συνέχεια περιγράφονται οι διάφορες διαδικασίες που αναπτύχθηκαν.

\subsection{Frequency Based Selection}
Αφαίρεση των λέξεων που εμφανίζονται πολύ συχνά ή πολύ σπάνια.
Η επιλογή βασίζεται μόνο στο αν μια λέξη εμφανίζεται ή όχι σε μια βιβλιοθήκη.
Ο απόλυτος αριθμός εμφανίσεων δεν έχει σημασία.
Έτσι, αν θέσουμε κατώτατο όριο $3$ και μία λέξη εμφανιστεί $500$ φορές σε μια βιβλιοθήκη αλλά πουθενά αλλού, τότε αυτή η λέξη θεωρείται ότι εμφανίζεται πολύ σπάνια και θα κοπεί.

Η υλοποίηση βρίσκεται στη συνάρτηση \lstinline!frequency_based_selection! και ακολουθούμε τα εξής βήματα:
\begin{enumerate}
\item Μετατροπή του dataset σε bool.
\begin{lstlisting}[numbers=none, aboveskip=\smallskipamount, belowskip=\smallskipamount, captionpos=none]
to_bool = dataset.applymap(lambda x: True if x else False)
\end{lstlisting}

\item Άθροισμα κατά γραμμή.
\begin{lstlisting}[numbers=none, aboveskip=\smallskipamount, belowskip=\smallskipamount, captionpos=none]
to_bool_sums = to_bool.sum(axis=0)
\end{lstlisting}

\item Εύρεση και αφαίρεση όσων στηλών είναι εκτός των ορίων από το αρχικό dataset \lstinline!low_bound! και \lstinline!upper_bound!
\begin{lstlisting}[numbers=none, aboveskip=\smallskipamount, belowskip=\smallskipamount, captionpos=none]
to_drop = [
    column
    for column, nonzeros in zip(columns, to_bool_sums)
    if nonzeros < low_bound or nonzeros > upper_bound
]
return dataset.drop(to_drop, axis=1)
\end{lstlisting}
\end{enumerate}

\subsection{Join Duplicates}
Συνάθροιση των λέξεων που είναι διπλές.
Αυτή η συνάρτηση συνήθως έχει νόημα αν μετατρέψουμε το αρχείο σε lower case καθώς το διαβάζουμε.
\begin{lstlisting}[captionpos=none, numbers=none]
def join_duplicates(dataset):
    """Join duplicate words."""
    return dataset.groupby(dataset.columns, axis=1).sum()
\end{lstlisting}

\subsection{Μετατροπή σε δυαδικό}
Όποτε μια λέξη δεν εμφανίζεται η τιμή παραμένει μηδέν.
Αλλιώς γίνεται 1.
\begin{lstlisting}[captionpos=none, numbers=none]
def bool_it(dataset):
    """Convert all int values too boolean."""
    return dataset.applymap(lambda x: 1 if x else 0)
\end{lstlisting}

\subsection{Drop Fry Words}
Αφαίρεση των \href{http://www.k12reader.com/subject/vocabulary/fry-words/}{Fry Words}.
Αποτελεί μια λίστα από τις $1000$ πιο συχνές λέξεις της Αγγλικής γλώσσας.
Καθώς αυτές οι λέξεις είναι πολύ γενικές δεν έχουν κάποιο ιδιαίτερο νόημα σε προγραμματιστικό περιβάλλον και έτσι τις αφαιρούμε.
Οι λέξεις φορτώνονται από ένα αρχείο και συγκρίνονται με τις λέξεις του dataset.
\begin{lstlisting}[captionpos=none, numbers=none, breaklines=true]
def drop_fry_words(dataset, filename='fry-words.txt'):
    """
    Drops columns that have a name that is a Fry word.
    The Fry Sight Word List is made up of the most frequently used words in
    children's books, novels, articles and textbooks.
    """
    with open(filename) as file_object:
        fry_words = []
        for line in file_object:
            fry_words += filter_line(line, delimiter=' ', startpos=0)
    to_drop = [word for word in fry_words if word in dataset.columns]
    return dataset.drop(to_drop, axis=1)
\end{lstlisting}

\subsection{Join Similar}
Συνάθροιση όλων των λέξεων που είναι παρόμοιες σαν strings (με βάση τους χαρακτήρες).
Υλοποιείται στην συνάρτηση \lstinline!join_similar!.

Η δομή της συνάρτησης είναι η εξής:
\begin{itemize}
\item Καθώς αυτού του τύπου σύγκριση μπορεί να δώσει αποτελέσματα που δεν θέλουμε να συναθροίσουμε έπρεπε να ελέγξουμε όλα τα αποτελέσματα
και να εξαιρέσουμε μερικά από αυτά μέσω της \lstinline!list! \lstinline!blacklist!.
\begin{lstlisting}[captionpos=none, numbers=none, breaklines=true]
blacklist = [
    ('adding', 'padding'),
    ...
    ('stats', 'status')
]
\end{lstlisting}

\item Στην κυρίως επανάληψη της συνάρτησης συγκρίνουμε κάθε λέξη με τις υπόλοιπες.
Αυτές που είναι αρκετά όμοιες τις ομαδοποιούμε μαζί.
\begin{lstlisting}[captionpos=none, numbers=none, breaklines=true]
for idx, word in enumerate(dataset.columns[:-1]):
    rest = dataset.columns[idx + 1:]
    ...
    for match in close:
        if (word, match) in blacklist:
            close.remove(match)
    if close:
        to_join.append([word] + close)
        to_drop += close
\end{lstlisting}

\item Για την εύρεση της ομοιότητας χρησιμοποιείται η συνάρτηση \lstinline!get_close_matches()! της βιβλιοθήκης \lstinline!difflib! της python.
\begin{lstlisting}[captionpos=none, numbers=none, breaklines=true]
close = get_close_matches(
    word=word,  # For which word to find similarities.
    possibilities=rest,  # search in the rest of the columns list.
    n=len(rest),  # Don't limit the search for too many results.
    cutoff=similarity_bound)  # At least this score.
\end{lstlisting}

\item Συναθροίζουμε όλες τις ομάδες παρόμοιων string στο πρώτο μέλος της ομάδας και αφαιρούμε τα υπόλοιπα.
Οι συναθροίσεις γίνονται από το τέλος έτσι ώστε αν
η λέξη \texttt{A} μοιάζει με την \texttt{B} και η \texttt{B} με την \texttt{C} στο τελικό αποτέλεσμα θα
συναθροίσουμε τις \texttt{B} και \texttt{C} στην \texttt{A}.
\begin{lstlisting}[captionpos=none, numbers=none, breaklines=true]
for group in to_join[::-1]:
    # sum group to the first member
    dataset[group[0]] = sum(dataset[member] for member in group)
return dataset.drop(to_drop, axis=1)
\end{lstlisting}
\end{itemize}

\subsection{Gibberish Detector}
\sloppy Για την εύρεση λέξεων που είναι "ασυναρτησίες" χρησιμοποιούμε την συνάρτηση
\lstinline!gibberish_detector()!.

Η συνάρτηση χρησιμοποιεί το πρόγραμμα \href{https://github.com/rrenaud/Gibberish-Detector}{Gibberish-Detector}
που βρέθηκε μέσω της ερώτησης \href{http://stackoverflow.com/a/6298193/3430986}{"Is there any way to detect strings like putjbtghguhjjjanika?"}
στο \href{stackoverflow.com}{stackoverflow}.
Το πρόγραμμα αυτό χρησιμοποιεί μια μαρκοβιανή αλυσίδα για να πετύχει τον στόχο του και είναι επίσης γραμμένο σε Python(2).
Για να το χρησιμοποιήσουμε πρέπει να τρέξουμε το \texttt{gib\_detect\_train.py} για την εκπαίδευση
και να μετακινήσουμε το αποτέλεσμα \texttt{gib\_model.pki} στον φάκελο με τα datasets.

Στην συνάρτηση αφαιρούμε κάθε λέξη του dataset που ταξινομείται ως "ασυναρτησία".
\begin{lstlisting}[captionpos=none, numbers=none, breaklines=true]
to_drop = [
    column for column in dataset.columns if is_word_gibberish(column)]
return dataset.drop(to_drop, axis=1)
\end{lstlisting}
