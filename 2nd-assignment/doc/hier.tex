\subsection{Περιγραφή εξερεύνησης των ιεραρχικών μοντέλων}
Τα μοντέλα μας δημιουργήθηκαν με την χρήση του Matlab και η διαδικασία που περιγράφτηκε στην εισαγωγή πραγματοποιείται στο αρχείο \texttt{optimizer\_hier.m}.
Το script \texttt{scripth} δημιουργεί τα σχήματα που παρουσιάζονται σε αυτή την ενότητα.
για τις διάφορες παραμέτρους. Οι αποστάσεις ανάμεσα στα σημεία ή υπολογίζονται για τους ιεραρχικούς με την χρήση της συνάρτησης \lstinline!pdist!, η οποία μπορεί να υπολογίσει τις εξής αποστάσεις:
\begin{itemize}
    \item \lstinline!'euclidean'!
    \item \lstinline!'seuclidean'!
    \item \lstinline!'Mankowski'!
    \item \lstinline!'chebychev'!
    \item \lstinline!'mahalanobis'!
    \item \lstinline!'cosine'!
    \item \lstinline!'correlation'!
    \item \lstinline!'spearman'!
    \item \lstinline!'jaccard'!
\end{itemize}

Περισσότερες πληροφορίες για την \lstinline!pdist!
\href{http://www.mathworks.com/help/stats/pdist.html}{εδώ}.
Σαν αποστάσεις ικανοποιητικά αποτελέσματα έδιναν μόνο το correlation και το 
cosine μία πιθανή εξήγηση βρίσκεται  στη σελίδα \pageref{metrics}

Έπειτα, χρησιμοποιήθηκε η συνάρτηση \lstinline!linkage! οποία δημιουργεί ουσιαστικά το δέντρο ιεραρχίας.
Η μετρική που χρησιμοποιείται για να συνδέσει τα κοντινότερα cluster μπορεί να είναι μία από της εξής:
\begin{itemize}
    \item \lstinline!'single'!
    \item \lstinline!'complete'!
    \item \lstinline!'average'!
    \item \lstinline!'weighted'!
    \item \lstinline!'centroid'!
    \item \lstinline!'median'!
    \item \lstinline!'ward'! 
\end{itemize}

Από άποψη χρόνου και αποτελεσμάτων ύστερα από δοκιμές για τα τελικά πειράματα επιλέχτηκαν οι weighted, ward, complete, average.
Έπειτα, με την χρήση της συνάρτησης cluster δημιουργήσαμε τα τελικά cluster που θέλαμε τα παραπάνω βήματα υλοποιούνται από τις παρακάτω γραμμές κώδικα 

\begin{lstlisting}[language=Matlab]
%simple example of hierarchical clustering
Y = pdist (X,distance); %X is an array containig the data
YY = squareform(Y); %convert Y in a square form
Z =linkage(YY,); %find  hierarchical cluster tree,
CDX = cluster(Z,'maxclust',8);
\end{lstlisting}

Επίσης, υπάρχει η συνάρτηση \lstinline!cophonet(Z,YY)! η οποία είναι μια μετρική που υπολογίζει την συσχέτιση ανάμεσα στις συνδέσεις που έχει δημιουργήσει το Z και τις αντίστοιχες αποστάσεις στο YY.
Όσο πιο μεγάλη είναι η συσχέτιση τόσο πιο καλά έχει αποτυπωθεί η διαφορετικότητα των αρχικών σημείων, σαν συνδέσεις μεταξύ clusters, περισσότερα
\href{https://en.wikipedia.org/wiki/Cophenetic_correlation}{εδώ}.

\section{Πειραματικά αποτελέσματα}
Χρησιμοποιούνται οι γνωστές μετρικές $Silhouette$, $Cohesion$,$Separation$
αλλά και η $Success Rate 1$ η οποία είναι μία από τις 2 μετρικές που  υπολογίζει το πόσα πετύχαμε από τα πραγματικά δεδομένα.Περισσότερα για το πως υπολογίστηκαν και τη αντιπροσωπεύει η success1 στο τέλος του κεφαλαίου.    

Έγινε χρήση των προαναφερθέντων αλγορίθμων απόστασης και εφαρμόστηκαν στον καθένα 4 διαφορετικοί τρόποι σύνδεσης για όλα τα dataset.

\noindent\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={figure}}
    \includegraphics[width=\linewidth]{images/hierCosBar.pdf}
    \captionof{figure}{Μετρικές για τον ιεραρχικό αλγόριθμο με την χρήση του Cosine}
    \label{fig:CosineHier}
\end{minipage}

\noindent\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={figure}}
    \includegraphics[width=\linewidth]{images/hierCorBar.pdf}
    \captionof{figure}{Μετρικές για τον ιεραρχικό αλγόριθμο με την χρήση του Cosine}
    \label{fig:hierCorBar}
\end{minipage}

Βλέπουμε ότι και στις 2 περιπτώσεις ότι όσο υψηλότερο είναι το ποσοστό επιτυχίας,
τόσο υψηλότερο είναι η μετρική $Silhouette$ και η $Separation$.
Αντίθετα, η $Cohesion$ όσο αυξάνεται το ποσοστό επιτυχίας είναι μικρότερη.
Αυτά ήταν και τα επιθυμητά αποτελέσματα.
Η μετρικές μας να βελτιώνονται όταν πετυχαίνουμε καλύτερα αποτελέσματα. 
Τα καλύτερα ποσοστά σημειώνονται από τις μεθόδους average και weighted
και όσο αφορά τα dataset βλέπουμε ότι υπάρχουν διαφορές αν και μικρές
Να σημειώσουμε ότι το αρχικό dataset δεν συμπεριλαμβάνεται σε αυτές τις μετρήσεις.

\noindent\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={figure}}
    \includegraphics[width=\linewidth]{images/hierCorBar.pdf}
    \captionof{figure}{Μετρικές για τον ιεραρχικό αλγόριθμο με την χρήση του Cosine}
    \label{fig:CorrelationHier}
\end{minipage}

To dataset που έδωσε καλύτερο (αν και υπήρχαν ισοβαθμίες) $Silhouette$ και $Success Rate 1$ ήταν το ίδιο:
\url{join_duplicates/freq_8_70/gibberish_detector/join_similar/dataset.csv-average}.

\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={table}}
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{lllllllll}
            Dataset & Method & Distance Type & Number of Clusters & Cohesion & Separation & Silhouette & Success Rate & Max Type \\
            Α & average & Cosine & 8 & 0.351 & 0.504 & 0.229 & 0.787 & Best Success Rate \\
            A & average & Correlation & 8 & 0.360 & 0.504 & 0.259 & 0.231& Best Silhouette \\
        \end{tabular}
    }
    \captionof{table}{Συνοπτικός πίνακας καλύτερων αποτελεσμάτων ιεραρχικών}
    \label{table:best-hier}
\end{minipage}

Επίσης, παραθέτουμε το σχήμα που προέκυψε από το weka και δείχνει ποιες
βιβλιοθήκες μπήκαν σε ποια Clusters (Προέκυψε το ίδιο και για best $Success Rate 1$ και για $Silhouette$ , λογικά υπήρχαν αμοιβαίες ανταλλαγές ανάμεσα στα clusters)

\noindent\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={figure}}
    \includegraphics[width=\linewidth]{images/hier_result.eps}
    \captionof{figure}{Libraries σε Clusters}
    \label{fig:clustering}
\end{minipage}

Βλέπουμε ότι η τα \texttt{swing-libraries} (κόκκινα) τα \texttt{http-clients} (γκρι), 
και τα \texttt{command-line-parsers} (λαχανί) τα πετύχαμε ακριβώς με ενώ χάθηκαν 
λίγα από τα android σε γειτονικά clusters.to Cluster 4 έμεινε μόνο με 2 στοιχεία ενώ το 3 είχε μεγάλη ανομοιογένεια το 2 είχε την πλειοψηφία των \texttt{xml} (φουξ), \texttt{json}(ροζ), \texttt{csv} (γαλάζιο).
Μπορούμε να χαρακτηρίσουμε αυτό το αποτέλεσμα ώς μέτριο προς καλό
καθώς πετύχαμε ακριβώς 3 από τα 8 clusters και μία σχετική πλειοψηφία από τα υπόλοιπα.

Τέλος, παραθέτουμε τον πίνακα ομοιότητας και το δεντροδιάγραμμα για το καλύτερο dataset και clustering.

\noindent\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={figure}}
    \includegraphics[width=\linewidth]{images/heatHier.eps}
    \captionof{figure}{Το δεντροδιάγραμμα με τα καλύτερα αποτελέσματα}
    \label{fig:Heat}
\end{minipage}

Είναι εμφανές ότι οι μετρική της απόστασης που διαλέξαμε είναι σωστή
καθώς τα clusters διαφαίνονται σχετικά με το μάτι.

\noindent\begin{minipage}{\linewidth}
    \centering
    \captionsetup{type={figure}}
    \includegraphics[width=\linewidth]{images/dentroHier.eps}
    \captionof{figure}{Το δεντροδιάγραμμα με τα καλύτερα αποτελέσματα}
    \label{fig:Dentro}
\end{minipage}

Τέλος, να προσθέσουμε ότι το $coph\_coeff = 0.7067$ για το συγκεκριμένο δεντροδιάγραμμα και πίνακα ομοιότητας.
