\chapter{Εκτέλεση Αλγορίθμων}


Αφού ολοκληρώσαμε την επεξεργασία των δεδομένων μας (dataset) εφαρμόσαμε τους αλγορίθμους που αναφέραμε παραπάνω για την επίλυση του προβλήματος. Για κάθε αλγόριθμο υπολογίσαμε τις μετρικές του πάνω στο σετ εκπαίδευσης (training set)και στο τέλος συγκρίνουμε την απόδοση των αλγορίθμων. Οι αλγόριθμοι που χρησιμοποιήσαμε ανήκουν στις κατηγορίες Bayes, Trees, SVM και NNs.


\section{Bayes}
O Κανόνας ταξινόμησης του Βayes(για 2 κλάσεις) μπορεί να εκφραστεί ως εξής:\\
\begin{center}
Αν $P(\omega_1|\mathbf{x}) > P(\omega_2|\mathbf{x})$ το $\mathbf{x} $ ταξινομείται στην κλάση 
$\omega_1$\\
Αν $P(\omega_1|\mathbf{x}) < P(\omega_2|\mathbf{x})$ το $\mathbf{x} $ ταξινομείται στην κλάση 
$\omega_2$
\end{center}

Η θεωρεία αποφάσεων κατά Bayes χρησιμοποιεί προφανώς  τον βασικό κανόνα της θεωρεία πιθανοτήτων , 
τον κανόνα του Bayes ο οποίος είναι : 
\begin{align*}
P(\omega_i|x) = \frac{p(\mathbf{x}|\omega_i)P(\omega_i)}{p(\mathbf{x})}
\end{align*}

\begin{center}
όπου η $p(x)$ είναι η συνάρτηση πυκνότητας πιθανότητας του  \textbf{x} 
\end{center}

Οπότε για να ταξινομήσουμε το κάθε διάνυσμα πρέπει να υπολογίσουμε το $p(\mathbf{x}|\omega_i)$ και το
 $P(\omega_i) $ .Το $ P(\omega_i)$ συνήθως το Λαμβάνουμε ως $\frac{N_i}{N}$. To κύριο πρόβλημα έγκειται
στον υπολογισμό των $p(\mathbf{x}|\omega_i)$.Υπάρχουν διάφοροι τρόποι για να υπολογιστεί ακριβώς
 αυτή  η σ.π.π όπως η MAL και η  MAP. Η δυσκολία βρίσκεται στο γεγονός ότι όσα περισσότερα features
 (διάσταση του  \textbf{x}) υπάρχουν τόσο περισσότερα σημεία εκπαίδευσης θα χρειαστούμε για να τις 
 προσεγγίσουμε με επιθυμητή ακρίβεια. Υπάρχει η γνωστή κατάρα της διαστατικότητας(curse of 
 dimensionality).Η οποία χονδρικά μπορεί να εκφραστεί ως : Αν χρειαζόμαστε Ν σημεία για καλή ακρίβεια σε 
 1 διάσταση αν έχουμε l διαστάσεις χρειαζόμαστε $N^l$ σημεία . Λύση  σε αυτό το πρόβλημα δίνουν οι 
 αλγόριθμοι NaiveBayes και BayesNet.
 
 Στον  Naivebayes υποθέτουμε ότι τα επιμέρους features είναι ανεξάρτητα μεταξύ τους .Έτσι έχουμε  $p(\mathbf{x}|\omega_i) = \prod_{j=1}^lp{x_j}{\omega_i} $Τα σημεία εκπαίδευσης που  θα χρειαζόταν 
 είναι  $lN$ .Επειδή έχουμε Ν μονοδιάσταστες σ.π.π  
 
Στα BayesΝet γίνεται κάτι το ενδιάμεσο ανάμεσα στον NaiveBayes και τoυς αλγορίθμους προσέγγισης της ακριβής σ.π.π . Βασιζόμενος στον κανόνα της αλυσίδας \\
\begin{align*}
p(x_1,x_2 \ldots ,x_l)  = p(x_l|x_{l-1},\ldots,x_1)p(x_{l-1}||x_{l-2},\ldots,x_1),\ldots,p(x_2|x_1)p(x_1)
\end{align*} 
Οι συγκεκριμένοι αλγόριθμοι προσπαθούν αν βρουν μερικά μόνα features που είναι ανεξάρτητα μεταξύ τους και να προσεγγίσουν με βάση αυτή την (μερική ανεξαρτησία) τις σ.π.π .Όσο πιο σωστά προσδιρίσουν πια χαρακτηριστικά είναι ανεξάρτητα μεταξύ τους τόσο πιο καλή είναι η προσέγγιση της κανονική σ.π.π .Αυτές η σχέσεις ανεξαρτησίας-εξάρτησης συμβολίζονται με τα BayesNets τα οποία είναι πάντα DAG (directed acyclic gragh).Αν έχουμε το bayesNet τότε αυτό σημαίνει πρακτικά πως κάθε feature  είναι στατιστικά ανεξάρτητο από οποιοδήποτε συνδυασμό τον μη απογόνων του δοθέντων όμως τον γονέων του 
\begin{center}
	\includegraphics{Bayesnet2.png}
	\captionof{figure}{BayesNet example}
	\label{fig:BayesNet}  
\end{center}

\section{Trees}
\begin{center}
    \includegraphics{decision-trees.eps}
    \captionof{figure}{Decision Trees}
    \label{fig:decision-tree}
\end{center}
Ένα δέντρο απόφασης είναι μια δομή που περιλαμβάνει ένα κεντρικό κόμβο (στην κορυφή), κλαδιά και φύλλα.
Κάθε εσωτερικός κόμβος υποδηλώνει μια δοκιμή σε ένα χαρακτηριστικό,
κάθε κλάδος δηλώνει το αποτέλεσμα μιας δοκιμής
και κάθε φύλλο περιέχει το τελικό αποτέλεσμα που είναι η κλάση στην οποία ανήκουν τα δεδομένα (classification tree).

Για την κατασκευή χρησιμοποιήθηκαν ο αλγόριθμος J48 που είναι μια υλοποίηση του C4.5 στο weka
και η μέθοδος random forest που ουσιαστικά χρησιμοποιεί πολλά δέντρα απόφασης για να παράγει καλύτερα αποτελέσματα.
\input{algorithms/j48}
\input{algorithms/random-forest}
\section{Support Vector Machine (SVM)}


Γενική Ιδέα: H γενική ιδέα της μεθόδου SVM είναι η εύρεση ενός υπερεπίπεδου το οποίο θα διαχωρίζει τα δεδομένα. Το υπερεπίπεδο αυτό καλείται όριο απόφασης. 


\begin{figure}[h!]
	\includegraphics{SVMphoto.png}
	\caption {SVM example}
	\label{fig:SVM}  
\end{figure}

Όπως βλέπουμε και από το Σχήμα 3.1 παραπάνω τα δεδομένα μας χωρίζονται σε 2 κλάσεις. Βρίσκουμε τα
 Support Vector των 2 κλάσεων που είναι τα δεδομένα των 2 κλάσεων που απέχουν την μικρότερη απόσταση
  μεταξύ και προσπαθούμε να βρούμε το υπερεπίπεδο το οποίο μεγιστοποιεί το περιθώριο (margin) μεταξύ
   των Support Vector. Ουσιαστικά στον SVM θέλουμε να μεγιστοποιήσουμε αυτό το περιθώριο.


Ο αλγόριθμος SVM περιγράφεται μαθηματικά από τις παρακάτω εξισώσεις:
\newline
Θέλουμε να μεγιστοποιήσουμε την ποσότητα   $ Margin=\frac{2}{|w\|^2} $
\newline
Ισοδύναμα θέλουμε να ελαχιστοποιήσουμε την ποσότητα  $ L(w)=\frac{|w\|^2}{2} $
\newline
	Αλλά δεδομένων των παρακάτω περιορισμών:


 $f(x_i)$ 
$ = \begin{cases} 1, & \mbox{if } w\cdot\mbox{ $x_i +b \geq 1 $} \\ -1, & \mbox{if } w\cdot\mbox{ $x_i +b \leq -1 $} \end{cases} $


 Ουσιαστικά πρόκειται για ένα πρόβλημα βελτιστοποίησης υπό περιορισμούς.
 Το πρόβλημα βελτιστοποίησης έχει λύση της μορφής :
\newline $\boldsymbol  w $=$\sum a_i y_i \boldsymbol x_i $ $b$=$y_k$-$\boldsymbol w^T \boldsymbol x_k$ για κάθε $\boldsymbol x_k$ τέτοιο ώστε $a_k\not= 0$
 
 Για κάθε μη-μηδενικό $a_i$ συνεπάγεται ότι το $\boldsymbol x_i$ είναι Support Vector
 
 Η συνάρτηση ταξινόμησης που χρησιμοποιεί ο SVM έχει την μορφή:
\begin{align*} 
f(x)=\sum a_i y_i \boldsymbol {x_i}^T \boldsymbol x_i +b
\end{align*}

Ο SVM είναι ιδιαίτερα αποδοτικός όταν τα δεδομένα εισόδου είναι γραμμικά διαχωρίσιμα. Ωστόσο μπορούμε να
 εφαρμόσουμε τον αλγόριθμο και στην περίπτωση που τα δεδομένα εισόδου δεν είναι γραμμικά διαχωρίσιμα
 χρησιμοποιώντας την τεχνική "kernel trick". Με αυτήν την τεχνική ουσιαστικά τοποθετούμε τα δεδομένα εισόδου σε μεγαλύτερο χώρο διαστάσεων χωρίς να υπολογίσουμε αναλυτικά τις συντεταγμένες των δεδομένων αυτών. Πρόκειται λοιπόν για μια αποδοτική τεχνική που μας επιτρέπει την χρήση του SVM σε κάθε περίπτωση.

Εξετάστηκαν 2 διαφορετικοί αλγόριθμοι SVM.Οι αλγόριθμοι που θα χρησιμοποιήσουμε στο Weka είναι ο SMO και ο LibSVM.O Sequential minimal optimization(SMO) σπάει το πρόβλημα σε μικρότερα προβλήματα τα οποία λύνονται αναλυτικά. Χρησιμοποιούνται για την επίλυση προβλήματων quadratic programming .Το βασικό πλεονέκτημα του SMO είναι ότι είναι ελάχιστα ταχύτερο στην εκπαίδευση. O LibSVM είναι ο πιο δημοφιλής τρόπος για να εκπαιδεύσεις SVM.

\input{algorithms/smo-rbf-pca}
\input{algorithms/libsvm}
 
\section{KNN}

Γενική Ιδέα: Η μέθοδος αυτή βασίζεται στην εξεύρεση των k κοντινότερων γειτονικών σημείων του δείγματος προς ταξινόμηση.

\begin{center}
	\includegraphics{Knnphoto.png}
	\captionof{figure}{KNN example}
	\label{fig:KNN}  
\end{center}

Όπως βλέπουμε και στο παραπάνω σχήμα το δείγμα προς ταξινόμηση συμβολίζεται με το αστεράκι.Βλέπουμε ένα παράδειγμα για k=3 όπου συλλέγουμε τους 3 κοντινότερους γείτονες και όμοια για k=6.

Ο ταξινομητής ΚΝΝ απαιτεί:
\begin{enumerate}
	\item Το σετ εκπαίδευσης
	\item Μια μετρική απόστασης όπου συνήθως χρησιμοποιείται η Ευκλείδια Απόσταση
	\item Η παράμετρος k που δηλώνει τον αριθμό των k γειτόνων που θα χρησιμοποιηθεί
\end{enumerate}

Η πιο σημαντική παράμετρος που πρέπει να επιλεγεί είναι ο αριθμός των γειτόνων(k παράμετρος).Μικρές τιμές του k κάνουν τον ταξινομητή μας ευαίσθητο στον θόρυβο ενώ πολύ μεγάλες τιμές μπορούν να κάνουν την γειτονιά μας να περιέχει και σημεία από άλλες κλάσεις τα οποία όμως βρίσκονται πολύ μακριά από το δειγμα προς ταξινόμηση.
\input{algorithms/ibk-k-1}
\input{algorithms/ibk}
