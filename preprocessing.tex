\chapter{Προ-επεξεργασία Δεδομένων}
Σε αυτό το στάδιο προσπαθήσαμε , τόσο εποπτικά όσο και μέσω φίλτρων του Weka, να επεξεργαστούμε τα δεδομένα μας και να δημιουργήσουμε νέα σετ εκπαίδευσης που είχαν καλύτερες αποδόσεις ή να απομακρύνουμε διάφορα γνωρίσματα τα οποία δεν παρείχαν ουσιαστική πληροφορία για την ταξινόμησή μας
\section {Επιλογή γνωρισμάτων(Feature selection) }
To feature classId είναι προφανές ότι δεν περιέχει κάποια πληροφορία για το αν η κλάση μας έχει μεγάλη πιθανότητα σφάλματος ή όχι.Για αυτό το λόγω το αφαιρούμε χωρίς περεταίρω ανάλυση , καθώς μπορεί να προκαλέσει προβλήματα over-fitting.
Για τα υπόλοιπα γνωρίσματα θα χρησιμοποιήσουμε 2 από τους εκτιμητές  του weka για να προσδιορίσουμε την χρησιμότητά τους .Το ChisquaredAttributeEval και το infoGainAttributeΕval
\subsection{ChisquaredAttributeEval}

\begin{tabular}{r|l}
Ranked& attributes\\
  \hline
215.7338 &  wmc\\
207.4892 &   numberOfLinesOfCode\\
190.9737 &   fanOut\\
180.9782 &  rfc\\
166.2854 &   cbo\\
 88.3387 &  numberOfMethods\\
 80.068  &   numberOfAttributes\\
 78.8778&   lcom\\
 68.9736&   numberOfPublicAttributes\\
 65.4263&   numberOfPrivateMethods\\
 53.8219&   numberOfPublicMethods\\
 42.069  &   fanIn\\
 21.7091&    noc\\
  0      &  numberOfPrivateAttributes\\
  0     &   numberOfMethodsInherited\\
  0    &     dit\\
  0    &     numberOfAttributesInherited\\
 \end{tabular}

  \newpage
 
 Υλοποιεί το γνωστό τεστ $\chi^2 $ στο οποίο κάνουμε την μηδενική υπόθεση ότι  2 χαρακτηριστικά ενός δείγματος από ένα πληθυσμό(εδώ είναι οι κλάσεις)    είναι ανεξάρτητα μεταξύ τους.
Στη συγκεκριμένη περίπτωση Θα ελέγξει κατά πόσο το κάθε (feature έχει εξάρτηση από την δυαδική τιμή bug)
Θα ελέγξει δηλαδή  κατά πόσο ισχύει η γνωστή σε όλους σχέση $P(X=x \cap Bugs = b) = P(X=x)\times P(Bugs =b)$
Όπου το X=x σημαίνει το feature Χ ναι πάρει την τιμή χ και το b  παίρνει τις τιμές 1,0 και προφανώς σημαίνει η κλάση να είναι εσφαλμένη η όχι .Προφανώς είναι επιθυμητό να μην ισχύει η στατιστική ανεξαρτησία που περιγράφεται από την παραπάνω σχέση για τα feature μας και την κλάση ταξινόμησης.Το συγκεκριμένο test έδωσε τα εξής αποτελέσματα :

\subsection{infoGainAttributeΕval}
Αυτός εκτιμητής υπολογίζει την αξία ενός χαρακτηριστικού μετρώντας το πληροφοριακό κέρδος που έχουμε σε σχέση με την κλάση χρησιμοποιώντας τη σχέση  $H(Class) - H(Class | Attribute)$.Όπου H είναι η εντροπία 
Ο συγκεκριμένος έδωσε τα εξής αποτελέσματα :\\


\begin{tabular}{r|l}
 Ranked & attribute\\
\hline
 0.1701 &  wmc\\
 0.1667&   numberOfLinesOfCode\\
 0.1529&   fanOut\\
 0.1384&   cbo\\
 0.1379&   rfc\\
 0.072 &  numberOfMethods\\
 0.0642&   lcom\\
 0.0599&   numberOfAttributes\\
 0.0501&   numberOfPublicAttributes\\
 0.0473&  numberOfPrivateMethods\\
 0.0414&   numberOfPublicMethods\\
 0.0306&   fanIn\\
 0.0183&   noc\\
 0      &  numberOfPrivateAttributes\\
 0      &  numberOfMethodsInherited\\
 0      &  dit\\
 0      &  numberOfAttributesInherited\\
 \end{tabular}\newline \\ \\ 
Βλέπουμε ότι και οι 2 εκτιμητές τα μας έδειξαν ότι       τα features :
\begin{itemize}
\bfseries
\item numberOfPrivateAttributes
\item numberOfMethodsInherited
\item dit
\item numberOfAttributesInherited 
\end{itemize}
 Δεν σχετίζονται με το αν η κλάση μας έχει bug ή όχι οπότε επιλέγουμε νατα αφαιραίσουμε καια υτά 
 για να μειώσουμε τον κίνδυνο overtraining αλλά και την επιπλεόν πολυπλοκότητα που δημιουργούν τα πολλά feature
 \section { Extreme values και Outliers }
 Μία συνήθης τεχνική είναι να αφαιρούμε δείγματα από το training set τα οποία  έχουν ακραίες τιμές  .Αυτά τα δείγματα συνήθως θεωρούνται ότι έχουν μετρηθεί εσφαλμένα  ή ότι είναι εξαιρέσεις που αντιβαίνουν τον κανόνα και μας δυσκολεύουν στο να κάνουμε σωστή ταξινόμηση . Όταν  όμως χρησιμοποιήσαμε το φίλτρο ΙnterquantileRange του weka για να αφαιρέσουμε τις extreme και outliers τιμές παρατηρήσαμε ότι σχεδόν οι μισές classes me bug αφαιρέθηκαν ενώ δεν συνέβη το ίδιο με τις κλάσεις που δεν είχαν bug.Οπότε υποθέσαμε ότι  υπήρχε κάποια εξάρτηση ανάμεσα στα δείγματα που είχαν extreme τιμές και τις κλάσεις με bug.Για αυτό το λόγο δημιουργήσαμε 2 καινούργια feature το Extreme και το Outliers.Η παραπάνω υπόθεση επιβεβαιώθηκε τόσο θεωρητικά τρέχοντας τους εκτιμητές που αναφέρθηκαν παραπάνω και παίρνοντας εκτιμήσεις που τα κατατάσσεται στη μέση σχεδόν των υπόλοιπων feature. Επίσης σε μερικούς αλγόριθμους πήραμε καλύτερα αποτελέσματα από κάνοντας χρήση αυτών Και τέλος μπορούμε να το δούμε και εποπτικά από το σχήμα που ακολουθεί
 \begin{figure}[h!]
	\centering
	\subfloat[ExtremeValues(no-yes)]{\includegraphics[width=0.5\textwidth, height = 0.3\textheight]{ExtremeValues.eps}}
	\subfloat[OutliersValues(no-yes)]{\includegraphics[width=0.5\textwidth , height = 0.3\textheight]{OutLiersValue.eps}}
	\caption{Τα δείγματα με extreme και outliers}
\end{figure}

Φαίνεται καθαρά ότι η αναλογία τον bugs/nobugs είναι μεγαλύτερη στην δεξιά μπάρα(yes) σε σχέση μεν την αριστερή(no)
 \section { Διακριτοποίηση - Κανονικοποίηση  }
 2 άλλες τεχνικές που χρησιμοποιήθηκαν είναι  η διακριτοποίηση και η κανονικοποίσηση .Διάφοροι αλγόριθμοι βελτιώνονται όταν τα χαρακτηριστικά τους είναι διακριτοποιημένα (Bayes) ή κανονικοποιημένα(SVM)
 H Διακριτοποίηση έγινε με το supervised φίλτρο discretized το οποίο διακριτοποιεί το κάθε feature ξεχωριστά με το βέλτιστο τρόπο που θεωρεί αυτό.Ενώ η κανονικοποίηση έγινε με το unsupervised φίλτρο normalize το οποίο απλά έκαν το range των τιμών να είναι από 0 εως 1.παρατείθεται μόνο παράδειγma από την διακριτοποίηση καθώς υπάρχει οπτική αλλάγη σε αντίθεση με τhn κανονικοποίηση.
 
  \begin{figure}[h!]
	\centering
	\subfloat[Undisritized Num\_Lines\_of\_Code]{\includegraphics[width=0.5\textwidth, height = 0.3\textheight]{undiscretized.eps}}
	\subfloat[disritized Num\_Lines\_of\_Code]{\includegraphics[width=0.5\textwidth , height = 0.3\textheight]{discretized.eps}}
	\caption{Τα δείγματα με extreme και outliers}
\end{figure}
 \section { Principal component analysis (PCA)}
 \section{Τελικά training sets}